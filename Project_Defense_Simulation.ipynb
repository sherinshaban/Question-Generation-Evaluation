{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1iNYaAiRqrsRvqHN4FQsL5wUGLIcYBGzc",
      "authorship_tag": "ABX9TyN4Jba9CODgA9YpGH3MTjOg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "db17378ceec4479a885e4264aba5e7aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_76ff757e63a6447ab20db9509c2f9a9f",
              "IPY_MODEL_a22983e93d2243868483a054f0f5f6d2",
              "IPY_MODEL_289ac925a78f4d098dba539a8c27d17a",
              "IPY_MODEL_ad451ae79a784f678d141a590114595d",
              "IPY_MODEL_cb9e536f6af3473f91d4b3dd581886a3"
            ],
            "layout": "IPY_MODEL_3ab9564ef5204732867f3b46c8c280e9"
          }
        },
        "76ff757e63a6447ab20db9509c2f9a9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_95d2f0a8fda64c72a50e2b4f528d376c",
            "placeholder": "​",
            "style": "IPY_MODEL_cc73de038d2d4f88b8cec4f5a38a7847",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "a22983e93d2243868483a054f0f5f6d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_423b7b8091d6420aa67af1e62a6aa4d1",
            "placeholder": "​",
            "style": "IPY_MODEL_74eb7de33e1e4507808115a73d46ed64",
            "value": ""
          }
        },
        "289ac925a78f4d098dba539a8c27d17a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_34a24cc4d7ca499f8b71f38547854ae4",
            "style": "IPY_MODEL_6f607b601f8d47189244fa80c144f14d",
            "value": true
          }
        },
        "ad451ae79a784f678d141a590114595d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_1cdc5f8c472c4ee4bf30a3123c9f9e38",
            "style": "IPY_MODEL_b2ad8bd6c1074ac7b623a3d4b09ae13c",
            "tooltip": ""
          }
        },
        "cb9e536f6af3473f91d4b3dd581886a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_13328415605c493b80c6f0c94a2b7dc4",
            "placeholder": "​",
            "style": "IPY_MODEL_5aeb57fae9af4bc49215dc91df4f0b81",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "3ab9564ef5204732867f3b46c8c280e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "95d2f0a8fda64c72a50e2b4f528d376c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc73de038d2d4f88b8cec4f5a38a7847": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "423b7b8091d6420aa67af1e62a6aa4d1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "74eb7de33e1e4507808115a73d46ed64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "34a24cc4d7ca499f8b71f38547854ae4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f607b601f8d47189244fa80c144f14d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1cdc5f8c472c4ee4bf30a3123c9f9e38": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b2ad8bd6c1074ac7b623a3d4b09ae13c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "13328415605c493b80c6f0c94a2b7dc4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5aeb57fae9af4bc49215dc91df4f0b81": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5c6ec2663bcc4e6d86d199d3d2704ada": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_92e80cbb43744239b0da75d30cd9a28d",
              "IPY_MODEL_2ac755822bf449f782b2c3be327333fe",
              "IPY_MODEL_b2e36a57a4194a7396fc50abd0195058"
            ],
            "layout": "IPY_MODEL_6c4464e3ccac4663ab4b0a7332c957e7"
          }
        },
        "92e80cbb43744239b0da75d30cd9a28d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c9fe3cf98d594b9f9b176e15c8376d23",
            "placeholder": "​",
            "style": "IPY_MODEL_a97b30df551e43829aa2e9e41a9c8b77",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "2ac755822bf449f782b2c3be327333fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_30a4775094e640e7a5420f27cff532e1",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b9528919a9ed47b089b41f2f108fb077",
            "value": 3
          }
        },
        "b2e36a57a4194a7396fc50abd0195058": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_967d8f9c4c2e41a0b42d943daab3e1a2",
            "placeholder": "​",
            "style": "IPY_MODEL_aa222f8e756044228410b98f6677654d",
            "value": " 3/3 [01:03&lt;00:00, 21.36s/it]"
          }
        },
        "6c4464e3ccac4663ab4b0a7332c957e7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c9fe3cf98d594b9f9b176e15c8376d23": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a97b30df551e43829aa2e9e41a9c8b77": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "30a4775094e640e7a5420f27cff532e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b9528919a9ed47b089b41f2f108fb077": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "967d8f9c4c2e41a0b42d943daab3e1a2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa222f8e756044228410b98f6677654d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sherinshaban/Question-Generation-Evaluation/blob/main/Project_Defense_Simulation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5kcEiFxU8BZ",
        "outputId": "57988e67-d809-4a33-b77e-f8f637c78505"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.35.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.10)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.10.5)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (0.2.1)\n",
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n",
            "Collecting python-docx\n",
            "  Downloading python_docx-1.2.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from python-docx) (5.4.0)\n",
            "Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.12/dist-packages (from python-docx) (4.15.0)\n",
            "Downloading python_docx-1.2.0-py3-none-any.whl (252 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-docx\n",
            "Successfully installed python-docx-1.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install torch\n",
        "!pip install transformers\n",
        "!pip install sentencepiece\n",
        "!pip install PyPDF2\n",
        "!pip install python-docx"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U transformers accelerate huggingface_hub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0w_1DEdcjYOD",
        "outputId": "bcc89c16-be02-426d-db0b-ea6180765c6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.10.1)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.12/dist-packages (0.35.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.8.0+cu126)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (1.1.10)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.4.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.10.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "import PyPDF2\n",
        "import docx\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "from huggingface_hub import login\n",
        "import os\n",
        "import json\n",
        "\n",
        "from google.colab import drive\n",
        "try:\n",
        "    drive.mount('/content/drive')\n",
        "except Exception:\n",
        "    print(\"Drive already mounted or error occurred during mounting.\")\n",
        "\n",
        "DRIVE_PATH = '/content/drive/MyDrive/Project_Files_For_Graduation/'\n",
        "\n",
        "CHUNK_SIZE = 800\n",
        "MAX_PAGES = 30\n",
        "\n",
        "def read_pdf(file_path, max_pages):\n",
        "    \"\"\"Reads text content from a PDF file.\"\"\"\n",
        "    text = \"\"\n",
        "    try:\n",
        "        with open(file_path, 'rb') as file:\n",
        "            reader = PyPDF2.PdfReader(file)\n",
        "            num_pages = min(len(reader.pages), max_pages)\n",
        "            for page in reader.pages[:num_pages]:\n",
        "                text += page.extract_text() or \"\"\n",
        "    except FileNotFoundError:\n",
        "        return f\"Error: The file at {file_path} was not found.\"\n",
        "    except Exception as e:\n",
        "        return f\"An error occurred while reading the file: {e}\"\n",
        "    return text\n",
        "\n",
        "def read_docx(file_path, max_pages):\n",
        "    \"\"\"Reads text content from a DOCX file.\"\"\"\n",
        "    text = \"\"\n",
        "    try:\n",
        "        doc = docx.Document(file_path)\n",
        "        limit_words = max_pages * 500\n",
        "        words_count = 0\n",
        "        for para in doc.paragraphs:\n",
        "            para_text = para.text + \"\\n\"\n",
        "            text += para_text\n",
        "            words_count += len(para_text.split())\n",
        "            if words_count >= limit_words:\n",
        "                break\n",
        "    except FileNotFoundError:\n",
        "        return f\"Error: The file at {file_path} was not found.\"\n",
        "    except Exception as e:\n",
        "        return f\"An error occurred while reading the file: {e}\"\n",
        "    return text\n",
        "\n",
        "def split_text(text, chunk_size):\n",
        "    \"\"\"Splits a large text into smaller chunks.\"\"\"\n",
        "    words = text.split()\n",
        "    chunks = []\n",
        "    for i in range(0, len(words), chunk_size):\n",
        "        chunks.append(\" \".join(words[i:i + chunk_size]))\n",
        "    return chunks\n",
        "\n",
        "def generate_questions_with_mistral(text, prompt, model, tokenizer):\n",
        "    \"\"\"\n",
        "    Generates questions using the Mistral-7B-Instruct model with a specific prompt.\n",
        "    NOTE: This function now accepts the model and tokenizer objects.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Create a prompt to instruct the model\n",
        "        full_prompt = f\"\"\"\n",
        "        [INST]\n",
        "        {prompt}\n",
        "\n",
        "        Project Content:\n",
        "        {text}\n",
        "        [/INST]\n",
        "        \"\"\"\n",
        "        # Tokenize the prompt and generate questions\n",
        "        encoded_input = tokenizer(full_prompt, return_tensors=\"pt\", add_special_tokens=False).to(model.device)\n",
        "        generated_ids = model.generate(\n",
        "            **encoded_input,\n",
        "            max_new_tokens=4000,\n",
        "            do_sample=True,\n",
        "            top_p=0.95,\n",
        "            temperature=0.7,\n",
        "            num_return_sequences=1\n",
        "        )\n",
        "\n",
        "        # Decode the generated output and clean it up\n",
        "        output = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "        start_index = output.find(\"[/INST]\")\n",
        "        if start_index != -1:\n",
        "            questions_text = output[start_index + len(\"[/INST]\"):].strip()\n",
        "            # Split the output by lines, remove empty lines, and return a list\n",
        "            questions_list = [q.strip() for q in questions_text.split('\\n') if q.strip()]\n",
        "            return questions_list\n",
        "        else:\n",
        "            return [\"No questions could be generated. The model response was not as expected.\"]\n",
        "\n",
        "    except Exception as e:\n",
        "        return [f\"An error occurred during model inference: {e}\"]\n",
        "\n",
        "def generate_essay_questions_only(text, model, tokenizer):\n",
        "    \"\"\"Generates exactly 7 in-depth essay questions.\"\"\"\n",
        "    all_questions = {}\n",
        "\n",
        "    essay_prompt = \"Generate exactly 7 in-depth essay questions that require a detailed, comprehensive answer about the project's core concepts or impact. Provide a sample model answer for each question.\"\n",
        "\n",
        "    all_questions['Essay Questions'] = generate_questions_with_mistral(text, essay_prompt, model, tokenizer)\n",
        "\n",
        "    return all_questions\n",
        "\n",
        "def save_questions_to_json(questions_dict, file_name=\"essay_questions_and_answers.json\"):\n",
        "    \"\"\"Saves the generated questions and answers to a JSON file in Google Drive.\"\"\"\n",
        "\n",
        "    full_path = os.path.join(DRIVE_PATH, file_name)\n",
        "\n",
        "    try:\n",
        "\n",
        "        os.makedirs(os.path.dirname(full_path), exist_ok=True)\n",
        "\n",
        "        with open(full_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(questions_dict, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "        print(f\"Essay questions and answers have been successfully saved to the file: {full_path}\")\n",
        "    except IOError as e:\n",
        "        print(f\"An error occurred while trying to save the file: {e}\")\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function to run the script.\"\"\"\n",
        "    file_path = input(\"Please enter the path to your PDF or DOCX file (e.g., /content/drive/MyDrive/Project.pdf): \")\n",
        "\n",
        "    if file_path.endswith('.pdf'):\n",
        "        document_text = read_pdf(file_path, MAX_PAGES)\n",
        "    elif file_path.endswith('.docx'):\n",
        "        document_text = read_docx(file_path, MAX_PAGES)\n",
        "    else:\n",
        "        print(\"Error: Unsupported file type. Please use a .pdf or .docx file.\")\n",
        "        return\n",
        "\n",
        "    if document_text.startswith(\"Error\"):\n",
        "        print(document_text)\n",
        "        return\n",
        "\n",
        "    print(\"\\n-----------------------------------------------------\")\n",
        "    print(\"Generating only 7 Essay Questions with Mistral...\")\n",
        "    print(\"-----------------------------------------------------\")\n",
        "\n",
        "    # --- Load the model and tokenizer ONCE ---\n",
        "    try:\n",
        "        login()\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_id, dtype=torch.float16, device_map=\"auto\")\n",
        "\n",
        "    text_chunks = split_text(document_text, CHUNK_SIZE)\n",
        "\n",
        "    all_questions = {\n",
        "        'Essay Questions': []\n",
        "    }\n",
        "\n",
        "    for i, chunk in enumerate(text_chunks):\n",
        "        print(f\"Processing chunk {i+1}/{len(text_chunks)}...\")\n",
        "        chunk_questions = generate_essay_questions_only(chunk, model, tokenizer)\n",
        "\n",
        "        if 'Essay Questions' in chunk_questions:\n",
        "             all_questions['Essay Questions'].extend(chunk_questions['Essay Questions'])\n",
        "\n",
        "    save_questions_to_json(all_questions)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 651,
          "referenced_widgets": [
            "db17378ceec4479a885e4264aba5e7aa",
            "76ff757e63a6447ab20db9509c2f9a9f",
            "a22983e93d2243868483a054f0f5f6d2",
            "289ac925a78f4d098dba539a8c27d17a",
            "ad451ae79a784f678d141a590114595d",
            "cb9e536f6af3473f91d4b3dd581886a3",
            "3ab9564ef5204732867f3b46c8c280e9",
            "95d2f0a8fda64c72a50e2b4f528d376c",
            "cc73de038d2d4f88b8cec4f5a38a7847",
            "423b7b8091d6420aa67af1e62a6aa4d1",
            "74eb7de33e1e4507808115a73d46ed64",
            "34a24cc4d7ca499f8b71f38547854ae4",
            "6f607b601f8d47189244fa80c144f14d",
            "1cdc5f8c472c4ee4bf30a3123c9f9e38",
            "b2ad8bd6c1074ac7b623a3d4b09ae13c",
            "13328415605c493b80c6f0c94a2b7dc4",
            "5aeb57fae9af4bc49215dc91df4f0b81",
            "5c6ec2663bcc4e6d86d199d3d2704ada",
            "92e80cbb43744239b0da75d30cd9a28d",
            "2ac755822bf449f782b2c3be327333fe",
            "b2e36a57a4194a7396fc50abd0195058",
            "6c4464e3ccac4663ab4b0a7332c957e7",
            "c9fe3cf98d594b9f9b176e15c8376d23",
            "a97b30df551e43829aa2e9e41a9c8b77",
            "30a4775094e640e7a5420f27cff532e1",
            "b9528919a9ed47b089b41f2f108fb077",
            "967d8f9c4c2e41a0b42d943daab3e1a2",
            "aa222f8e756044228410b98f6677654d"
          ]
        },
        "id": "N2V7WCV5WqNX",
        "outputId": "9421627b-da48-4070-f863-e8f359d83587"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Please enter the path to your PDF or DOCX file (e.g., /content/drive/MyDrive/Project.pdf): /content/drive/MyDrive/Project_Files_For_Graduation/E-Learning Platform gra.pro idea (5).pdf\n",
            "\n",
            "-----------------------------------------------------\n",
            "Generating only 7 Essay Questions with Mistral...\n",
            "-----------------------------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "db17378ceec4479a885e4264aba5e7aa"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5c6ec2663bcc4e6d86d199d3d2704ada"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing chunk 1/1...\n",
            "Essay questions and answers have been successfully saved to the file: /content/drive/MyDrive/Project_Files_For_Graduation/essay_questions_and_answers.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "import PyPDF2\n",
        "import docx\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "from huggingface_hub import login\n",
        "from google.colab import drive\n",
        "import torch\n",
        "\n",
        "# NEW LIBRARIES FOR FAST SIMILARITY EVALUATION\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "# Connect Google Drive\n",
        "try:\n",
        "    drive.mount('/content/drive')\n",
        "except Exception:\n",
        "    print(\"Drive already mounted or error occurred during mounting.\")\n",
        "\n",
        "DRIVE_PATH = '/content/drive/MyDrive/Project_Files_For_Graduation/'\n",
        "QUESTIONS_FILE_NAME = \"essay_questions_and_answers.json\"\n",
        "\n",
        "# Settings for the Similarity Model\n",
        "SIMILARITY_MODEL_NAME = 'paraphrase-multilingual-mpnet-base-v2'\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "\n",
        "def load_questions_from_json(file_name=QUESTIONS_FILE_NAME):\n",
        "    \"\"\"Loads the generated questions and answers from a JSON file in Google Drive.\"\"\"\n",
        "    full_path = os.path.join(DRIVE_PATH, file_name)\n",
        "    try:\n",
        "        with open(full_path, 'r', encoding='utf-8') as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "            if 'Essay Questions' in data:\n",
        "                 return data\n",
        "            else:\n",
        "                 print(\"Error: JSON file structure is missing 'Essay Questions' key.\")\n",
        "                 return None\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Questions file not found at {full_path}. Please generate questions first.\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while loading the file: {e}\")\n",
        "        return None\n",
        "\n",
        "def parse_qa_string(qa_string):\n",
        "    \"\"\"\n",
        "    Splits a question/model answer string based on the separator (Model Answer: or Correct Answer:).\n",
        "    \"\"\"\n",
        "    match = re.search(r'(.*?)(Model Answer|Correct Answer|True or False)\\s*:?\\s*(.*)', qa_string, re.DOTALL | re.IGNORECASE)\n",
        "\n",
        "    if match:\n",
        "        question = match.group(1).strip()\n",
        "        model_answer = match.group(3).strip()\n",
        "        question = re.sub(r'^\\d+\\.?\\s*Question\\s*\\d*\\s*:?\\s*', '', question, flags=re.IGNORECASE).strip()\n",
        "        return question, model_answer\n",
        "    else:\n",
        "        return qa_string.strip(), \"N/A - Model Answer not found in expected format.\"\n",
        "\n",
        "def load_similarity_model():\n",
        "    \"\"\"Loads the fast Sentence Transformer model once and puts it on the correct device.\"\"\"\n",
        "    try:\n",
        "        similarity_model = SentenceTransformer(SIMILARITY_MODEL_NAME, device=DEVICE)\n",
        "        return similarity_model\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading similarity model: {e}\")\n",
        "        return None\n",
        "\n",
        "def evaluate_by_similarity(model_answer, student_answer, similarity_model):\n",
        "    \"\"\"\n",
        "    Evaluates the student's answer by measuring its cosine similarity\n",
        "    to the model answer using vector embeddings.\n",
        "    \"\"\"\n",
        "    if not similarity_model:\n",
        "        return 0, \"Error: Similarity model not loaded.\"\n",
        "\n",
        "    sentences = [model_answer, student_answer]\n",
        "    embeddings = similarity_model.encode(sentences)\n",
        "\n",
        "    similarity_score = cosine_similarity(embeddings[0].reshape(1, -1), embeddings[1].reshape(1, -1))[0][0]\n",
        "\n",
        "    score_out_of_5 = round(similarity_score * 5, 1)\n",
        "\n",
        "    if score_out_of_5 >= 4.5:\n",
        "        feedback = \"Excellent! Your answer is highly accurate and comprehensive (Similarity: {:.2f}).\".format(similarity_score)\n",
        "    elif score_out_of_5 >= 3.5:\n",
        "        feedback = \"Very Good. Your answer is largely correct, but slightly misses some key details (Similarity: {:.2f}).\".format(similarity_score)\n",
        "    elif score_out_of_5 >= 2.5:\n",
        "        feedback = \"Fair. Your answer shows a partial understanding of the concept (Similarity: {:.2f}).\".format(similarity_score)\n",
        "    else:\n",
        "        feedback = \"Needs improvement. Your answer has significant discrepancies or is incomplete (Similarity: {:.2f}).\".format(similarity_score)\n",
        "\n",
        "    return score_out_of_5, feedback\n",
        "\n",
        "def run_simulation(questions_dict, similarity_model):\n",
        "    \"\"\"Simulates the Q&A process, collects student answers, and evaluates them using the FAST method.\"\"\"\n",
        "\n",
        "    print(\"\\n=====================================================\")\n",
        "    print(\"Starting Project Defense Simulation (FAST EVALUATION) \")\n",
        "    print(\"=====================================================\")\n",
        "\n",
        "    target_q_type = 'Essay Questions'\n",
        "\n",
        "    questions_list = questions_dict[target_q_type]\n",
        "\n",
        "    print(f\"Displaying ALL {len(questions_list)} questions of type: {target_q_type}\")\n",
        "\n",
        "    for i, qa_string in enumerate(questions_list):\n",
        "\n",
        "        question, model_answer = parse_qa_string(qa_string)\n",
        "\n",
        "        print(f\"\\n--- Question {i+1} ---\")\n",
        "        print(f\"QUESTION: {question}\")\n",
        "\n",
        "        student_answer = input(\"Your Answer (Student): \")\n",
        "\n",
        "        print(\"\\n[FAST GRADER is calculating similarity score...]\")\n",
        "\n",
        "        score, feedback = evaluate_by_similarity(model_answer, student_answer, similarity_model)\n",
        "\n",
        "        print(\"\\n--- Evaluation Result ---\")\n",
        "        print(f\"SCORE: {score}/5\")\n",
        "        print(f\"FEEDBACK: {feedback}\")\n",
        "        print(f\"\\nModel Answer (Reference): {model_answer}\")\n",
        "        print(\"------------------------------------------\\n\")\n",
        "\n",
        "# =========================================================\n",
        "# === 4. Main Execution Function (Loads only Similarity model) ===\n",
        "# =========================================================\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function to run the script.\"\"\"\n",
        "\n",
        "\n",
        "    print(\"\\n Loading fast Similarity Model...\")\n",
        "    similarity_model = load_similarity_model()\n",
        "    if similarity_model is None:\n",
        "        return\n",
        "    print(\"Similarity Model loaded successfully.\")\n",
        "\n",
        "    # 3. Logic to Load Questions\n",
        "    all_questions = load_questions_from_json()\n",
        "\n",
        "    if all_questions is None:\n",
        "        print(\"\\nFATAL ERROR: Questions file not loaded. Cannot run simulation.\")\n",
        "        return\n",
        "\n",
        "    # 4. Run Simulation and Evaluation\n",
        "    if all_questions:\n",
        "        run_simulation(all_questions, similarity_model)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ail0sb1tmaFw",
        "outputId": "22d3d306-5608-403d-f3f3-edb0f9835145"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "\n",
            " Loading fast Similarity Model...\n",
            "Similarity Model loaded successfully.\n",
            "\n",
            "=====================================================\n",
            "Starting Project Defense Simulation (FAST EVALUATION) \n",
            "=====================================================\n",
            "Displaying ALL 7 questions of type: Essay Questions\n",
            "\n",
            "--- Question 1 ---\n",
            "QUESTION: Analyze the role of AI in the E-Learning Platform and discuss how it contributes to enhancing the educational experience for students and teachers. Provide specific examples of AI features and their impact on the learning process. (\n",
            "Your Answer (Student): hhhh\n",
            "\n",
            "[FAST GRADER is calculating similarity score...]\n",
            "\n",
            "--- Evaluation Result ---\n",
            "SCORE: 0.30000001192092896/5\n",
            "FEEDBACK: Needs improvement. Your answer has significant discrepancies or is incomplete (Similarity: 0.05).\n",
            "\n",
            "Model Answer (Reference): AI plays a crucial role in the E-Learning Platform by automating quiz generation, analyzing student performance, providing simplified explanations, and offering personalized recommendations. For instance, the automatic question generation feature using advanced NLP models like T5 or AraT5 simplifies the process for teachers and ensures that students receive quizzes tailored to their skill level. The real-time performance analytics allow teachers to monitor student progress and identify common mistakes, enabling them to focus on frequently misunderstood topics. Furthermore, the personalized feedback and recommendations provide students with targeted suggestions for improvement.)\n",
            "------------------------------------------\n",
            "\n",
            "\n",
            "--- Question 2 ---\n",
            "QUESTION: Discuss the significance of Natural Language Processing (NLP) and machine learning in the E-Learning Platform. Explain how these technologies contribute to the automatic generation of quizzes and personalized feedback for students. (\n",
            "Your Answer (Student): Natural Language Processing (NLP) and machine learning (ML) play a transformative role in modern E-Learning platforms, enhancing both the efficiency and personalization of digital education.\n",
            "\n",
            "[FAST GRADER is calculating similarity score...]\n",
            "\n",
            "--- Evaluation Result ---\n",
            "SCORE: 4.300000190734863/5\n",
            "FEEDBACK: Very Good. Your answer is largely correct, but slightly misses some key details (Similarity: 0.86).\n",
            "\n",
            "Model Answer (Reference): Natural Language Processing (NLP) and machine learning are essential components of the E-Learning Platform, as they enable the automatic generation of quizzes and personalized feedback for students. Advanced NLP models like T5 or AraT5 extract relevant information from the lesson content and generate various types of questions. These models also analyze student performance and provide simplified explanations for misunderstood concepts, ensuring that students receive accurate and targeted feedback.)\n",
            "------------------------------------------\n",
            "\n",
            "\n",
            "--- Question 3 ---\n",
            "QUESTION: Compare and contrast the E-Learning Platform's automatic question generation feature with traditional quiz creation methods. Discuss the benefits and drawbacks of each approach and explain why the E-Learning Platform's AI-powered feature is more effective in enhancing the learning experience. (\n",
            "Your Answer (Student): Limited Scalability: Difficult to create large volumes of questions quickly.  Bias and Inconsistency\n",
            "\n",
            "[FAST GRADER is calculating similarity score...]\n",
            "\n",
            "--- Evaluation Result ---\n",
            "SCORE: 1.899999976158142/5\n",
            "FEEDBACK: Needs improvement. Your answer has significant discrepancies or is incomplete (Similarity: 0.38).\n",
            "\n",
            "Model Answer (Reference): Traditional quiz creation methods involve teachers manually designing and creating questions, which can be time-consuming and labor-intensive. In contrast, the E-Learning Platform's automatic question generation feature uses advanced NLP models to extract information from the lesson content and generate questions. This approach offers several benefits, such as reducing the manual workload for teachers, ensuring that questions are aligned with the lesson material, and providing students with quizzes tailored to their skill level. However, it may also have some drawbacks, such as potential inaccuracies in question generation and the lack of human intuition in question design.)\n",
            "------------------------------------------\n",
            "\n",
            "\n",
            "--- Question 4 ---\n",
            "QUESTION: Explain how the E-Learning Platform's real-time performance analytics benefit both students and teachers. Discuss specific examples of how these analytics can be used to improve the learning experience and facilitate more effective teaching. (\n",
            "Your Answer (Student): Real-time performance analytics in E-Learning platforms provide instant insights into student progress, engagement, and comprehension. These analytics benefit both students and teachers by enabling smarter decisions, faster interventions, and more personalized learning experiences.\n",
            "\n",
            "[FAST GRADER is calculating similarity score...]\n",
            "\n",
            "--- Evaluation Result ---\n",
            "SCORE: 4.699999809265137/5\n",
            "FEEDBACK: Excellent! Your answer is highly accurate and comprehensive (Similarity: 0.94).\n",
            "\n",
            "Model Answer (Reference): The E-Learning Platform's real-time performance analytics provide valuable insights into student performance and learning progress, enabling both students and teachers to monitor progress and identify areas for improvement. For instance, teachers can use these analytics to identify common student mistakes and focus on frequently misunderstood topics, while students can use them to track their skill level and adjust their study habits accordingly. Moreover, real-time analytics can also help teachers tailor their teaching approaches to individual students and provide personalized feedback and recommendations.)\n",
            "------------------------------------------\n",
            "\n",
            "\n",
            "--- Question 5 ---\n",
            "QUESTION: Discuss the importance of Arabic language and dialect support in the E-Learning Platform. Explain how this feature enhances the accessibility and inclusivity of the platform and contributes to more effective learning outcomes. (\n",
            "Your Answer (Student): Arabic language and dialect support transforms E-Learning platforms into powerful tools for inclusive education. By respecting linguistic diversity, these platforms empower students, bridge educational gaps, and foster deeper learning across the Arab world.\n",
            "\n",
            "[FAST GRADER is calculating similarity score...]\n",
            "\n",
            "--- Evaluation Result ---\n",
            "SCORE: 4.5/5\n",
            "FEEDBACK: Excellent! Your answer is highly accurate and comprehensive (Similarity: 0.91).\n",
            "\n",
            "Model Answer (Reference): Arabic language and dialect support are crucial components of the E-Learning Platform, as they enable students to learn in their native language and dialect. This feature enhances the accessibility and inclusivity of the platform, as it caters to a wider range of students and ensures that they receive accurate and effective learning materials. Furthermore, automatic normalization and understanding of various dialects also facilitate more effective communication between teachers and students, ensuring that everyone can engage in the learning process effectively.)\n",
            "------------------------------------------\n",
            "\n",
            "\n",
            "--- Question 6 ---\n",
            "QUESTION: Compare and contrast the E-Learning Platform's adaptive quizzing feature with traditional testing methods. Discuss the benefits and drawbacks of each approach and explain why the E-Learning Platform's adaptive quizzing is more effective in enhancing the learning experience. (\n",
            "Your Answer (Student):     One-size-fits-all approach may not reflect individual learning needs      Can cause anxiety due to high stakes and rigid timing      Limited feedback and delayed results\n",
            "\n",
            "[FAST GRADER is calculating similarity score...]\n",
            "\n",
            "--- Evaluation Result ---\n",
            "SCORE: 2.299999952316284/5\n",
            "FEEDBACK: Needs improvement. Your answer has significant discrepancies or is incomplete (Similarity: 0.47).\n",
            "\n",
            "Model Answer (Reference): Traditional testing methods involve students taking quizzes or exams that are fixed and not tailored to their skill level. In contrast, the E-Learning Platform's adaptive quizzing feature generates quizzes tailored to each student's performance, ensuring that they receive questions that accurately reflect their current level of understanding. This approach offers several benefits, such as reducing test anxiety, increasing student engagement, and providing more accurate assessments of student knowledge. However, it may also have some drawbacks, such as potential inaccuracies in question generation and the lack of standardization in testing.)\n",
            "------------------------------------------\n",
            "\n",
            "\n",
            "--- Question 7 ---\n",
            "QUESTION: Discuss the potential ethical considerations and challenges associated with using AI in the E-Learning Platform. Explore issues related to data privacy, bias, and equity and discuss strategies for mitigating these risks and ensuring that the use of AI in education is ethical and effective. (\n",
            "Your Answer (Student): AI systems often collect and analyze large amounts of student data, including performance, behavior, and personal information. If not properly protected, this data could be misused or exposed\n",
            "\n",
            "[FAST GRADER is calculating similarity score...]\n",
            "\n",
            "--- Evaluation Result ---\n",
            "SCORE: 3.4000000953674316/5\n",
            "FEEDBACK: Fair. Your answer shows a partial understanding of the concept (Similarity: 0.68).\n",
            "\n",
            "Model Answer (Reference): The use of AI in the E-Learning Platform raises several ethical considerations and challenges related to data privacy, bias, and equity. For instance, the collection and use of student data for personalized recommendations and analytics may raise concerns about data privacy and security. Furthermore, the potential for bias in AI algorithms and the lack of diversity in training data may also limit the effectiveness and fairness of the platform. To mitigate these risks, it is essential to adopt transparent data practices, ensure that AI algorithms are regularly audited for bias and accuracy, and prioritize diversity and inclusivity in training data and user experiences.)\n",
            "------------------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xeQwx9c0njry"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}