{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1iNYaAiRqrsRvqHN4FQsL5wUGLIcYBGzc",
      "authorship_tag": "ABX9TyN4Jba9CODgA9YpGH3MTjOg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sherinshaban/Question-Generation-Evaluation/blob/main/Project_Defense_Simulation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f5kcEiFxU8BZ"
      },
      "outputs": [],
      "source": [
        "!pip install torch\n",
        "!pip install transformers\n",
        "!pip install sentencepiece\n",
        "!pip install PyPDF2\n",
        "!pip install python-docx"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U transformers accelerate huggingface_hub"
      ],
      "metadata": {
        "id": "0w_1DEdcjYOD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "import PyPDF2\n",
        "import docx\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "from huggingface_hub import login\n",
        "import os\n",
        "import json\n",
        "\n",
        "from google.colab import drive\n",
        "try:\n",
        "    drive.mount('/content/drive')\n",
        "except Exception:\n",
        "    print(\"Drive already mounted or error occurred during mounting.\")\n",
        "\n",
        "DRIVE_PATH = '/content/drive/MyDrive/Project_Files_For_Graduation/'\n",
        "\n",
        "CHUNK_SIZE = 800\n",
        "MAX_PAGES = 30\n",
        "\n",
        "def read_pdf(file_path, max_pages):\n",
        "    \"\"\"Reads text content from a PDF file.\"\"\"\n",
        "    text = \"\"\n",
        "    try:\n",
        "        with open(file_path, 'rb') as file:\n",
        "            reader = PyPDF2.PdfReader(file)\n",
        "            num_pages = min(len(reader.pages), max_pages)\n",
        "            for page in reader.pages[:num_pages]:\n",
        "                text += page.extract_text() or \"\"\n",
        "    except FileNotFoundError:\n",
        "        return f\"Error: The file at {file_path} was not found.\"\n",
        "    except Exception as e:\n",
        "        return f\"An error occurred while reading the file: {e}\"\n",
        "    return text\n",
        "\n",
        "def read_docx(file_path, max_pages):\n",
        "    \"\"\"Reads text content from a DOCX file.\"\"\"\n",
        "    text = \"\"\n",
        "    try:\n",
        "        doc = docx.Document(file_path)\n",
        "        limit_words = max_pages * 500\n",
        "        words_count = 0\n",
        "        for para in doc.paragraphs:\n",
        "            para_text = para.text + \"\\n\"\n",
        "            text += para_text\n",
        "            words_count += len(para_text.split())\n",
        "            if words_count >= limit_words:\n",
        "                break\n",
        "    except FileNotFoundError:\n",
        "        return f\"Error: The file at {file_path} was not found.\"\n",
        "    except Exception as e:\n",
        "        return f\"An error occurred while reading the file: {e}\"\n",
        "    return text\n",
        "\n",
        "def split_text(text, chunk_size):\n",
        "    \"\"\"Splits a large text into smaller chunks.\"\"\"\n",
        "    words = text.split()\n",
        "    chunks = []\n",
        "    for i in range(0, len(words), chunk_size):\n",
        "        chunks.append(\" \".join(words[i:i + chunk_size]))\n",
        "    return chunks\n",
        "\n",
        "def generate_questions_with_mistral(text, prompt, model, tokenizer):\n",
        "    \"\"\"\n",
        "    Generates questions using the Mistral-7B-Instruct model with a specific prompt.\n",
        "    NOTE: This function now accepts the model and tokenizer objects.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Create a prompt to instruct the model\n",
        "        full_prompt = f\"\"\"\n",
        "        [INST]\n",
        "        {prompt}\n",
        "\n",
        "        Project Content:\n",
        "        {text}\n",
        "        [/INST]\n",
        "        \"\"\"\n",
        "        # Tokenize the prompt and generate questions\n",
        "        encoded_input = tokenizer(full_prompt, return_tensors=\"pt\", add_special_tokens=False).to(model.device)\n",
        "        generated_ids = model.generate(\n",
        "            **encoded_input,\n",
        "            max_new_tokens=4000,\n",
        "            do_sample=True,\n",
        "            top_p=0.95,\n",
        "            temperature=0.7,\n",
        "            num_return_sequences=1\n",
        "        )\n",
        "\n",
        "        # Decode the generated output and clean it up\n",
        "        output = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "        start_index = output.find(\"[/INST]\")\n",
        "        if start_index != -1:\n",
        "            questions_text = output[start_index + len(\"[/INST]\"):].strip()\n",
        "            # Split the output by lines, remove empty lines, and return a list\n",
        "            questions_list = [q.strip() for q in questions_text.split('\\n') if q.strip()]\n",
        "            return questions_list\n",
        "        else:\n",
        "            return [\"No questions could be generated. The model response was not as expected.\"]\n",
        "\n",
        "    except Exception as e:\n",
        "        return [f\"An error occurred during model inference: {e}\"]\n",
        "\n",
        "def generate_essay_questions_only(text, model, tokenizer):\n",
        "    \"\"\"Generates exactly 7 in-depth essay questions.\"\"\"\n",
        "    all_questions = {}\n",
        "\n",
        "    essay_prompt = \"Generate exactly 7 in-depth essay questions that require a detailed, comprehensive answer about the project's core concepts or impact. Provide a sample model answer for each question.\"\n",
        "\n",
        "    all_questions['Essay Questions'] = generate_questions_with_mistral(text, essay_prompt, model, tokenizer)\n",
        "\n",
        "    return all_questions\n",
        "\n",
        "def save_questions_to_json(questions_dict, file_name=\"essay_questions_and_answers.json\"):\n",
        "    \"\"\"Saves the generated questions and answers to a JSON file in Google Drive.\"\"\"\n",
        "\n",
        "    full_path = os.path.join(DRIVE_PATH, file_name)\n",
        "\n",
        "    try:\n",
        "\n",
        "        os.makedirs(os.path.dirname(full_path), exist_ok=True)\n",
        "\n",
        "        with open(full_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(questions_dict, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "        print(f\"Essay questions and answers have been successfully saved to the file: {full_path}\")\n",
        "    except IOError as e:\n",
        "        print(f\"An error occurred while trying to save the file: {e}\")\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function to run the script.\"\"\"\n",
        "    file_path = input(\"Please enter the path to your PDF or DOCX file (e.g., /content/drive/MyDrive/Project.pdf): \")\n",
        "\n",
        "    if file_path.endswith('.pdf'):\n",
        "        document_text = read_pdf(file_path, MAX_PAGES)\n",
        "    elif file_path.endswith('.docx'):\n",
        "        document_text = read_docx(file_path, MAX_PAGES)\n",
        "    else:\n",
        "        print(\"Error: Unsupported file type. Please use a .pdf or .docx file.\")\n",
        "        return\n",
        "\n",
        "    if document_text.startswith(\"Error\"):\n",
        "        print(document_text)\n",
        "        return\n",
        "\n",
        "    print(\"\\n-----------------------------------------------------\")\n",
        "    print(\"Generating only 7 Essay Questions with Mistral...\")\n",
        "    print(\"-----------------------------------------------------\")\n",
        "\n",
        "    # --- Load the model and tokenizer ONCE ---\n",
        "    try:\n",
        "        login()\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_id, dtype=torch.float16, device_map=\"auto\")\n",
        "\n",
        "    text_chunks = split_text(document_text, CHUNK_SIZE)\n",
        "\n",
        "    all_questions = {\n",
        "        'Essay Questions': []\n",
        "    }\n",
        "\n",
        "    for i, chunk in enumerate(text_chunks):\n",
        "        print(f\"Processing chunk {i+1}/{len(text_chunks)}...\")\n",
        "        chunk_questions = generate_essay_questions_only(chunk, model, tokenizer)\n",
        "\n",
        "        if 'Essay Questions' in chunk_questions:\n",
        "             all_questions['Essay Questions'].extend(chunk_questions['Essay Questions'])\n",
        "\n",
        "    save_questions_to_json(all_questions)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "N2V7WCV5WqNX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "import PyPDF2\n",
        "import docx\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "from huggingface_hub import login\n",
        "from google.colab import drive\n",
        "import torch\n",
        "\n",
        "# NEW LIBRARIES FOR FAST SIMILARITY EVALUATION\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "# Connect Google Drive\n",
        "try:\n",
        "    drive.mount('/content/drive')\n",
        "except Exception:\n",
        "    print(\"Drive already mounted or error occurred during mounting.\")\n",
        "\n",
        "DRIVE_PATH = '/content/drive/MyDrive/Project_Files_For_Graduation/'\n",
        "QUESTIONS_FILE_NAME = \"essay_questions_and_answers.json\"\n",
        "\n",
        "# Settings for the Similarity Model\n",
        "SIMILARITY_MODEL_NAME = 'paraphrase-multilingual-mpnet-base-v2'\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "\n",
        "def load_questions_from_json(file_name=QUESTIONS_FILE_NAME):\n",
        "    \"\"\"Loads the generated questions and answers from a JSON file in Google Drive.\"\"\"\n",
        "    full_path = os.path.join(DRIVE_PATH, file_name)\n",
        "    try:\n",
        "        with open(full_path, 'r', encoding='utf-8') as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "            if 'Essay Questions' in data:\n",
        "                 return data\n",
        "            else:\n",
        "                 print(\"Error: JSON file structure is missing 'Essay Questions' key.\")\n",
        "                 return None\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Questions file not found at {full_path}. Please generate questions first.\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while loading the file: {e}\")\n",
        "        return None\n",
        "\n",
        "def parse_qa_string(qa_string):\n",
        "    \"\"\"\n",
        "    Splits a question/model answer string based on the separator (Model Answer: or Correct Answer:).\n",
        "    \"\"\"\n",
        "    match = re.search(r'(.*?)(Model Answer|Correct Answer|True or False)\\s*:?\\s*(.*)', qa_string, re.DOTALL | re.IGNORECASE)\n",
        "\n",
        "    if match:\n",
        "        question = match.group(1).strip()\n",
        "        model_answer = match.group(3).strip()\n",
        "        question = re.sub(r'^\\d+\\.?\\s*Question\\s*\\d*\\s*:?\\s*', '', question, flags=re.IGNORECASE).strip()\n",
        "        return question, model_answer\n",
        "    else:\n",
        "        return qa_string.strip(), \"N/A - Model Answer not found in expected format.\"\n",
        "\n",
        "def load_similarity_model():\n",
        "    \"\"\"Loads the fast Sentence Transformer model once and puts it on the correct device.\"\"\"\n",
        "    try:\n",
        "        similarity_model = SentenceTransformer(SIMILARITY_MODEL_NAME, device=DEVICE)\n",
        "        return similarity_model\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading similarity model: {e}\")\n",
        "        return None\n",
        "\n",
        "def evaluate_by_similarity(model_answer, student_answer, similarity_model):\n",
        "    \"\"\"\n",
        "    Evaluates the student's answer by measuring its cosine similarity\n",
        "    to the model answer using vector embeddings.\n",
        "    \"\"\"\n",
        "    if not similarity_model:\n",
        "        return 0, \"Error: Similarity model not loaded.\"\n",
        "\n",
        "    sentences = [model_answer, student_answer]\n",
        "    embeddings = similarity_model.encode(sentences)\n",
        "\n",
        "    similarity_score = cosine_similarity(embeddings[0].reshape(1, -1), embeddings[1].reshape(1, -1))[0][0]\n",
        "\n",
        "    score_out_of_5 = round(similarity_score * 5, 1)\n",
        "\n",
        "    if score_out_of_5 >= 4.5:\n",
        "        feedback = \"Excellent! Your answer is highly accurate and comprehensive (Similarity: {:.2f}).\".format(similarity_score)\n",
        "    elif score_out_of_5 >= 3.5:\n",
        "        feedback = \"Very Good. Your answer is largely correct, but slightly misses some key details (Similarity: {:.2f}).\".format(similarity_score)\n",
        "    elif score_out_of_5 >= 2.5:\n",
        "        feedback = \"Fair. Your answer shows a partial understanding of the concept (Similarity: {:.2f}).\".format(similarity_score)\n",
        "    else:\n",
        "        feedback = \"Needs improvement. Your answer has significant discrepancies or is incomplete (Similarity: {:.2f}).\".format(similarity_score)\n",
        "\n",
        "    return score_out_of_5, feedback\n",
        "\n",
        "def run_simulation(questions_dict, similarity_model):\n",
        "    \"\"\"Simulates the Q&A process, collects student answers, and evaluates them using the FAST method.\"\"\"\n",
        "\n",
        "    print(\"\\n=====================================================\")\n",
        "    print(\"Starting Project Defense Simulation (FAST EVALUATION) \")\n",
        "    print(\"=====================================================\")\n",
        "\n",
        "    target_q_type = 'Essay Questions'\n",
        "\n",
        "    questions_list = questions_dict[target_q_type]\n",
        "\n",
        "    print(f\"Displaying ALL {len(questions_list)} questions of type: {target_q_type}\")\n",
        "\n",
        "    for i, qa_string in enumerate(questions_list):\n",
        "\n",
        "        question, model_answer = parse_qa_string(qa_string)\n",
        "\n",
        "        print(f\"\\n--- Question {i+1} ---\")\n",
        "        print(f\"QUESTION: {question}\")\n",
        "\n",
        "        student_answer = input(\"Your Answer (Student): \")\n",
        "\n",
        "        print(\"\\n[FAST GRADER is calculating similarity score...]\")\n",
        "\n",
        "        score, feedback = evaluate_by_similarity(model_answer, student_answer, similarity_model)\n",
        "\n",
        "        print(\"\\n--- Evaluation Result ---\")\n",
        "        print(f\"SCORE: {score}/5\")\n",
        "        print(f\"FEEDBACK: {feedback}\")\n",
        "        print(f\"\\nModel Answer (Reference): {model_answer}\")\n",
        "        print(\"------------------------------------------\\n\")\n",
        "\n",
        "# =========================================================\n",
        "# === 4. Main Execution Function (Loads only Similarity model) ===\n",
        "# =========================================================\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function to run the script.\"\"\"\n",
        "\n",
        "\n",
        "    print(\"\\n Loading fast Similarity Model...\")\n",
        "    similarity_model = load_similarity_model()\n",
        "    if similarity_model is None:\n",
        "        return\n",
        "    print(\"Similarity Model loaded successfully.\")\n",
        "\n",
        "    # 3. Logic to Load Questions\n",
        "    all_questions = load_questions_from_json()\n",
        "\n",
        "    if all_questions is None:\n",
        "        print(\"\\nFATAL ERROR: Questions file not loaded. Cannot run simulation.\")\n",
        "        return\n",
        "\n",
        "    # 4. Run Simulation and Evaluation\n",
        "    if all_questions:\n",
        "        run_simulation(all_questions, similarity_model)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "ail0sb1tmaFw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xeQwx9c0njry"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}